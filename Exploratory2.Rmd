---
title: "Exploratory2"
output: 
        html_document:
                pandoc_args: [
                "+RTS", "-K64m",
                  "-RTS"
                 ]
---
In this article, I will explore the text file provided by SwiftKey by analyzing the basic statistics and the most frequently used words. Since we have several files with different languages, I will use one file "en_US.Twitter.txt"" as an example for the exploratary analysis. The others should have a similar way for exploration. 

First, I set up the directory and load in the data. 


```{r}
# unzip(zipfile="Coursera-SwiftKey.zip")
setwd("~/Desktop/R projects/WordPred/DataSci MileStone Project/final/en_US")
enBlog <- readLines("en_US.blogs.txt")
enNews <- readLines("en_US.news.txt")
enTwitter <- readLines("en_US.twitter.txt")
```

Then, I show the basic statistics of the data with summary which includes linecount. For the word count, I will show it after the cleaning-up of the file.

```{r, cache=TRUE,echo=TRUE}
summary(enBlog)
summary(enNews)
summary(enTwitter)
```


The loading usually takes a long time thus for exploration purpose, I will sample 5000 lines of the tweets and write them into another file.

```{r}
sampleSize <- 5000
enTwitter <- enTwitter[rbinom(sampleSize, length(enTwitter),0.5)]
```

Then, I show the basic statistics of the data with summary which includes linecount and how many lines of tweets have the word "and".

```{r, cache=TRUE,echo=TRUE}
d <- 'and'
c <- 'and that'
length(enTwitter)
#head(enTwitter)
twitter.d <- grep(d,enTwitter)
length(twitter.d)
twitter <- enTwitter[twitter.d]
length(twitter)
#head(twitter)
```


I will first load some packages which are useful for text mining and then
I clean the data by changing the capital letter to lower ones, deleting the punctuation and remove the numbers, white spaces, punctuations and etc.

```{r, cache=FALSE,echo=FALSE}
library(tm)
library(qdap)
library(R.utils)
doc.vec <- VectorSource(twitter)
doc.corpus <- Corpus(doc.vec)
```
```{r, cache=TRUE,echo=TRUE}
summary(doc.corpus)
doc.corpus <- tm_map(doc.corpus, tolower)
doc.corpus <- tm_map(doc.corpus, removePunctuation)
doc.corpus <- tm_map(doc.corpus, removeNumbers)
#doc.corpus <- tm_map(doc.corpus, removeWords,stopwords("english"))
#install.packages("SnowballC")
library(SnowballC)

#doc.corpus <- tm_map(doc.corpus,stemDocument)
doc.corpus <- tm_map(doc.corpus,stripWhitespace)
doc.corpus <- tm_map(doc.corpus,PlainTextDocument)
#inspect(doc.corpus[1])
```


Now I am ready to count the words in the text and find the most frequently used words in the file.
```{r,echo=FALSE,cache=TRUE}

corpus1 <- gsub("\\s+", " ", gsub("\n|\t", " ", doc.corpus))
corpus1.wrds <- as.vector(unlist(strsplit(strip(corpus1), " ")))
length(corpus1.wrds)
corpus1.Freq <- data.frame(table(corpus1.wrds))
corpus1.Freq$corpus1.wrds  <- as.character(corpus1.Freq$corpus1.wrds)
corpus1.Freq <- corpus1.Freq[order(-corpus1.Freq$Freq), ]
rownames(corpus1.Freq) <- 1:nrow(corpus1.Freq)
key.terms <- corpus1.Freq[corpus1.Freq$Freq>10, 'corpus1.wrds'] #key words to match on corpus 1

```

The most frequently used words and their corresponding frequencies are:
```{r,echo=TRUE}
key.terms
corpus1.Freq$Freq[corpus1.Freq$Freq>100]
```
Finally I draw a histogram for the words frequency and its summary.
```{r}

hist(corpus1.Freq$Freq[corpus1.Freq$Freq>100])
summary(corpus1.Freq)
```

In the next step, I will creat a probalistic model for the relationship between words. I will explore what are the most used word pairs for each word and rank the frequency of words being used after specific words. After that, I will build a predictive model by using statistical methods inclduing random forests, boosting or decision tree. A temtative approach is shown below.

install.packages("RWeka")

options(mc.cores=1)

library(RWeka)

BigramTokenizer <- function(x) {RWeka::NGramTokenizer(x,RWeka::Weka_control(min =2, max =2))}

tdm <- TermDocumentMatrix(doc.corpus, control = list(tokenize = BigramTokenizer))

inspect(tdm[1:3,1:10])

inspect(tdm[rownames(tdm) == c,1:10])

p <- paste("^",d," ",sep="")

index <- grep(p,rownames(tdm))

temp <- inspect(tdm[index,])

freqMat <- data.frame(apply(temp,1,sum))

words <- rownames(freqMat)[which.max(freqMat[,1])]

wordPred <- gsub(p,'',words)

